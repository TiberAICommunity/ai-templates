name: codellama-7b-instruct
title: CodeLlama 7B Instruct
model_id: codellama/CodeLlama-7b-instruct
description: "Meta's CodeLlama 7B Instruct model running on Text Generation Inference (TGI). Specialized for code generation and understanding across multiple programming languages."

license_notice: "This model is available under the Llama 2 Community License. Before using this model, please review the complete license, accept Meta's Llama 2 License, and ensure compliance with usage terms and conditions."

# TGI specific configuration
tgi: true
architecture_diagram: |
  flowchart LR
      Client([Client])
      Traefik[Traefik Proxy]
      Auth[Auth Service]
      TGI[TGI Service]

      Client --> Traefik
      Traefik --> Auth
      Auth --> Traefik
      Traefik --> TGI
      TGI --> Traefik
      Traefik --> Client

      subgraph Internal["Internal Network"]
          Traefik
          Auth
          TGI
      end

deployment_steps:
  - title: "Create Account"
    description: "Create a standard account on Intel Tiber AI Cloud"
    link: "https://cloud.intel.com"
    link_text: "Visit Intel Cloud Portal"

  - title: "Select Hardware"
    description: "Select an Intel Data Center Max Series GPU VM"

  - title: "Clone Repository"
    description: "Clone the TGI repository and navigate to it"
    code: |
      git clone https://github.com/tiberaicommunity/xpu_tgi
      cd xpu_tgi

  - title: "Generate Token"
    description: "Generate authentication token for secure access using the provided utility"
    code: "python utils/generate_token.py"

  - title: "Start Model"
    description: "Start the CodeLlama model service with caching enabled using your generated token"
    code: "VALID_TOKEN=YOUR_TOKEN ./start.sh CodeLlama-7b --cache-models"

  - title: "Access Options"
    description: "Choose one of these methods to interact with the model:"
    sub_steps:
      - title: "Local Access"
        description: "Use localhost directly if running on the same machine"
        code: |
          curl -X POST http://localhost:8000/generate \
            -H 'Content-Type: application/json' \
            -H 'Authorization: Bearer YOUR_TOKEN' \
            -d '{"inputs": "Write a Python function to sort a list:"}'

      - title: "SSH Tunnel"
        description: "Add '-L 8000:localhost:8000' to your SSH connection to access from your local machine"
        code: "ssh -L 8000:localhost:8000 guest@guest-ip user@your-vm-ip"

      - title: "Temporary URL"
        description: "Use the provided tunnel script to create a temporary public URL using cloudflare (eval only)"
        code: |
          # In a new terminal:
          ./tunnel.sh

          # Then use the generated cloudflare URL:
          curl -X POST https://YOUR-TUNNEL-URL/generate \
            -H 'Content-Type: application/json' \
            -H 'Authorization: Bearer YOUR_TOKEN' \
            -d '{"inputs": "Write a Python function to sort a list:"}'

key_features:
  - "üîí Token-based authentication with automatic ban after failed attempts"
  - "üö¶ Rate limiting (global: 10 req/s, per-IP: 10 req/s)"
  - "üõ°Ô∏è Security headers and IP protection"
  - "üîÑ Health monitoring and automatic recovery"
  - "üöÄ Optimized for Intel GPUs"

# Model specific information
context_window: "4096 tokens"
strengths:
  - "Code generation"
  - "Code understanding"
  - "Code modification"
  - "Multi-language support"

use_cases:
  - "Code completion"
  - "Bug fixing"
  - "Code explanation"
  - "Documentation generation"
  - "Code review suggestions"

configuration: |
  MODEL_NAME=codellama-7b-instruct-tgi
  MODEL_ID=codellama/CodeLlama-7b-instruct
  TGI_VERSION=2.4.0-intel-xpu
  PORT=8089
  SHM_SIZE=2g
  MAX_CONCURRENT_REQUESTS=1
  MAX_BATCH_SIZE=1
  MAX_TOTAL_TOKENS=4096
  MAX_INPUT_LENGTH=2048
  MAX_WAITING_TOKENS=10

# Tags for filtering
tags:
  - label: "Transformer"
    color: "#3B82F6"
  - label: "4K context"
    color: "#10B981"
  - label: "Instruction"
    color: "#8B5CF6"
  - label: "TGI"
    color: "#EF4444"
  - label: "Meta"
    color: "#0EA5E9"
  - label: "LLaMA"
    color: "#F59E0B"
  - label: "Code"
    color: "#EC4899" 
