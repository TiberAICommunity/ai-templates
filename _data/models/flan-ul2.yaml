name: flan-ul2
title: Flan-UL2
model_id: google/flan-ul2
description: "Google's Flan-UL2 model running on Text Generation Inference (TGI) on Intel XPU. Flan-UL2 excels at instruction-following tasks like translation, summarization, and question answering."

license_notice: "Before using this model, please review the license terms and usage rights on the [Hugging Face model page](https://huggingface.co/google/flan-ul2). Ensure your use case complies with the model's terms of use and check for any commercial usage restrictions."

# TGI specific configuration
tgi: true
architecture_diagram: |
  flowchart LR
      Client([Client])
      Traefik[Traefik Proxy]
      Auth[Auth Service]
      TGI[TGI Service]

      Client --> Traefik
      Traefik --> Auth
      Auth --> Traefik
      Traefik --> TGI
      TGI --> Traefik
      Traefik --> Client

      subgraph Internal["Internal Network"]
          Traefik
          Auth
          TGI
      end

key_features:
  - "üîí Token-based authentication with automatic ban after failed attempts"
  - "üö¶ Rate limiting (global: 10 req/s, per-IP: 10 req/s)"
  - "üõ°Ô∏è Security headers and IP protection"
  - "üîÑ Health monitoring and automatic recovery"
  - "üöÄ Optimized for Intel GPUs"

# Model specific information
context_window: "4096 tokens"
strengths:
  - "Instruction following"
  - "Translation tasks"
  - "Text summarization"
  - "Structured responses"

use_cases:
  - "Language translation"
  - "Text summarization"
  - "Question answering"
  - "Classification tasks"

configuration: |
  MODEL_NAME=flan-ul2-tgi
  MODEL_ID=google/flan-ul2
  TGI_VERSION=2.4.0-intel-xpu
  PORT=8083
  SHM_SIZE=8g
  MAX_CONCURRENT_REQUESTS=10
  MAX_BATCH_SIZE=1
  MAX_TOTAL_TOKENS=4096
  MAX_INPUT_LENGTH=2039
  MAX_WAITING_TOKENS=20

deployment_steps:
  - title: "Setup Prerequisites"
    description: "Create Intel Tiber AI Cloud account and select GPU Max VM"
    link: "https://cloud.intel.com"
    link_text: "Visit Intel Cloud Portal"

  - title: "Initial Setup"
    description: "Clone repository and generate your authentication token"
    code: |
      git clone https://github.com/tiberaicommunity/xpu_tgi && cd xpu_tgi
      python utils/generate_token.py
      # Save the generated token for next step

  - title: "Start Model"
    description: "Launch model service using your generated token"
    code: "VALID_TOKEN=YOUR_TOKEN ./start.sh Flan-UL2 --cache-models"

  - title: "Access Option: Local"
    description: "Use localhost directly if running on the same machine"
    code: |
      curl -X POST http://localhost:8000/generate \
        -H 'Content-Type: application/json' \
        -H 'Authorization: Bearer YOUR_TOKEN' \
        -d '{"inputs": "Summarize this text: The development of quantum computing represents a significant leap in computational capabilities..."}'

  - title: "Access Option: Local Tunnel"
    description: "Use SSH tunnel to access from your local machine"
    code: "ssh -L 8000:localhost:8000 guest@guest-ip user@your-vm-ip"

  - title: "Access Option: Remote (Eval Only)"
    description: "Create temporary cloudflare tunnel for remote access"
    code: |
      ./tunnel.sh  # Run in new terminal
      
      # Use the generated URL:
      curl -X POST https://YOUR-TUNNEL-URL/generate \
        -H 'Content-Type: application/json' \
        -H 'Authorization: Bearer YOUR_TOKEN' \
        -d '{"inputs": "Summarize this text: The development of quantum computing represents a significant leap in computational capabilities..."}'

# Tags for filtering
tags:
  - label: "Transformer"
    color: "#3B82F6"
  - label: "4K context"
    color: "#10B981"
  - label: "Instruction"
    color: "#8B5CF6"
  - label: "TGI"
    color: "#EF4444"
  - label: "Google"
    color: "#34D399"
  - label: "Translation"
    color: "#F472B6" 
