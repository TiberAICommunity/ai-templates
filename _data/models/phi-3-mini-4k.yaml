name: Phi-3-mini-4k
title: Phi-3 mini 4k
model_id: microsoft/Phi-3-mini-4k-instruct
description: "Microsoft's Phi-3-mini-4k Instruct model running on Text Generation Inference (TGI). A compact yet powerful model optimized for reasoning, coding, and structured tasks."

license_notice: "Before using this model, please review the license terms and usage rights on the Hugging Face model page."

# System Requirements and Scaling
requirements:
  minimum:
    - "Intel Data Center GPU Max (PVC)"
    - "Ubuntu 22.04 LTS"
    - "32GB System RAM"
    - "100GB Storage"
    - "Docker & Docker Compose"
  recommended:
    - "64GB+ System RAM for larger models"
    - "Multiple GPUs for running multiple models"

scaling_options:
  - "Single GPU: One model per GPU"
  - "Multi-GPU: Deploy different models on separate GPUs"
  - "Load Balancing: Automatic GPU selection based on availability"

additional_resources:
  - title: "GitHub Repository"
    url: "https://github.com/tiberaicommunity/xpu_tgi"
    description: "Source code and detailed documentation"
    features:
      - "Manual deployment guides"
      - "Multi-model deployment"
      - "Advanced configuration options"
      - "Troubleshooting guides"

  - title: "Advanced Use Cases"
    description: "Check the repository for:"
    items:
      - "Custom model configuration"
      - "Multiple model deployments"
      - "GPU management strategies"
      - "Production deployment guides"

deployment_modes:
  - title: "Quick Deploy"
    description: "Single command deployment (recommended for single models)"
    suitable_for: ["Testing", "Single model deployments", "Quick evaluation"]

  - title: "Manual Deploy"
    description: "Step-by-step deployment from repository"
    suitable_for: ["Multiple models", "Custom configurations", "Production setups"]
    link: "https://github.com/tiberaicommunity/xpu_tgi#manual-deployment"

# TGI specific configuration
tgi: true
architecture_diagram: |
  flowchart LR
      Client([Client])
      Traefik[Traefik Proxy]
      Auth[Auth Service]
      TGI[TGI Service]

      Client --> Traefik
      Traefik --> Auth
      Auth --> Traefik
      Traefik --> TGI
      TGI --> Traefik
      Traefik --> Client

      subgraph Internal["Internal Network"]
          Traefik
          Auth
          TGI
      end

key_features:
  - "üîí Token-based authentication with automatic ban after failed attempts"
  - "üö¶ Rate limiting (global: 10 req/s, per-IP: 10 req/s)"
  - "üõ°Ô∏è Security headers and IP protection"
  - "üîÑ Health monitoring and automatic recovery"
  - "üöÄ Optimized for Intel GPUs"

# Model specific information
context_window: "4096 tokens"
strengths:
  - "Code generation"
  - "Step-by-step reasoning"
  - "Math problems"
  - "Technical explanations"

use_cases:
  - "Writing and debugging code"
  - "Solving mathematical problems"
  - "Explaining technical concepts"
  - "Logical reasoning tasks"

deployment_steps:
  - title: "Setup Cloud Environment"
    description: "Create Intel Tiber AI Cloud account and provision a GPU Max VM"
    subsections:
      - title: "Create Account"
        description: "Sign up for Intel Tiber AI Cloud"
        code: |
          # Visit Intel Cloud Portal and create account
          https://cloud.intel.com
          
          # Select GPU Max VM with recommended specs:
          # - OS: Ubuntu 22.04
          # - GPU: Intel Data Center GPU Max
          # - Memory: 32GB+ recommended

      - title: "Connect to VM"
        description: "SSH into your provisioned VM"
        code: |
          ssh user@your-vm-ip
          # Replace user and your-vm-ip with your credentials

  - title: "Quick Deploy (Recommended)"
    description: "Deploy the model with a single command"
    code: |
      curl -sSL https://raw.githubusercontent.com/tiberaicommunity/xpu_tgi/main/quick-deploy.sh | bash -s -- Phi-3-mini-4k

  - title: "Access Options"
    description: "Multiple ways to access your deployed model"
    subsections:
      - title: "Local Access"
        description: "Direct access on the same machine"
        code: |
          # Your authentication token will be displayed after deployment
          # Use it in your requests:
          
          curl -X POST http://localhost:8000/generate \
            -H "Authorization: Bearer YOUR_TOKEN" \
            -H "Content-Type: application/json" \
            -d '{
              "inputs": "Write a Python function that calculates factorial:",
              "parameters": {
                "max_new_tokens": 150,
                "temperature": 0.2
              }
            }'

      - title: "Remote Access (Evaluation)"
        description: "Temporary public URL for testing"
        code: |
          # In a new terminal on the VM:
          ./tunnel.sh
          
          # Use the generated URL with your token:
          curl -X POST https://YOUR-TUNNEL-URL/generate \
            -H "Authorization: Bearer YOUR_TOKEN" \
            -H "Content-Type: application/json" \
            -d '{"inputs": "Hello!"}'

      - title: "SSH Tunnel (Recommended)"
        description: "Secure remote access through SSH"
        code: |
          # On your local machine:
          ssh -L 8000:localhost:8000 user@your-vm-ip
          
          # Then access via localhost:8000

  - title: "Managing Services"
    description: "Control and monitor your deployment"
    subsections:
      - title: "Check Status"
        code: "./tgi-status.sh"

      - title: "Stop Services"
        code: |
          ./service_cleanup.sh --all     # Stop all services
          ./service_cleanup.sh --gpu N   # Stop specific GPU

examples:
  - title: "Code Generation"
    code: |
      curl -X POST http://localhost:8000/generate \
        -H "Authorization: Bearer YOUR_TOKEN" \
        -H 'Content-Type: application/json' \
        -d '{
          "inputs": "Write a Python function that calculates the factorial of a number. Include comments explaining the logic:",
          "parameters": {
            "max_new_tokens": 150,
            "temperature": 0.2
          }
        }'

  - title: "Step-by-Step Math"
    code: |
      curl -X POST http://localhost:8000/generate \
        -H "Authorization: Bearer YOUR_TOKEN" \
        -H 'Content-Type: application/json' \
        -d '{
          "inputs": "Solve step by step: A store offers a 15% discount on a $80 item. What is the final price after tax if the tax rate is 8%?",
          "parameters": {
            "max_new_tokens": 200,
            "temperature": 0.1
          }
        }'

best_practices:
  - "Use temperature 0.1-0.2 for code and math"
  - "Include 'step by step' for complex problems"
  - "For coding tasks, specify programming language, desired functionality, and any specific requirements"
  - "Break down complex queries into smaller, logical steps"

# Tags for filtering
tags:
  - label: "Transformer"
    color: "#3B82F6"
  - label: "4K context"
    color: "#10B981"
  - label: "Instruction"
    color: "#8B5CF6"
  - label: "TGI"
    color: "#EF4444"
