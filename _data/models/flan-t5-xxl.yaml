name: flan-t5-xxl
title: FLAN-T5 XXL
model_id: google/flan-t5-xxl
description: "Google's FLAN-T5-XXL running on Text Generation Inference (TGI). A powerful model specifically designed for instruction-following tasks with structured outputs. For multi-GPU deployments and advanced configurations, please visit our GitHub repository."

license_notice: "Before using this model, please review the license terms and usage rights on the Hugging Face model page. This deployment supports single and multi-GPU setups - see <a href='https://github.com/tiberaicommunity/xpu_tgi' target='_blank'>our repository</a> for advanced deployment options."

context_window: "512 tokens"
hardware_required: "Intel Max Series GPU (1100) VM - supports single/multi GPU deployments"

strengths:
  - "Structured tasks"
  - "Classification"
  - "Summarization"
  - "Translation"

use_cases:
  - "Text classification"
  - "Summarization"
  - "Question answering"
  - "Translation"
  - "Structured generation"

tgi: true
architecture_diagram: |
  flowchart LR
      Client([Client])
      Traefik[Traefik Proxy]
      Auth[Auth Service]
      TGI[TGI Service]

      Client --> Traefik
      Traefik --> Auth
      Auth --> Traefik
      Traefik --> TGI
      TGI --> Traefik
      Traefik --> Client

      subgraph Internal["Internal Network"]
          Traefik
          Auth
          TGI
      end

key_features:
  - "üîí Token-based authentication with automatic ban after failed attempts"
  - "üö¶ Rate limiting (global: 10 req/s, per-IP: 10 req/s)"
  - "üõ°Ô∏è Security headers and IP protection"
  - "üîÑ Health monitoring and automatic recovery"
  - "üöÄ Optimized for Intel GPUs with multi-GPU support"

deployment_steps:
  - title: "1. Setup Prerequisites"
    description: "Create Intel Tiber AI Cloud account, select GPU Max VM, and connect to your instance."
    link: "https://cloud.intel.com"
    link_text: "Visit Intel Cloud Portal"

  - title: "2. Quick Deploy (Recommended)"
    description: "Deploy the model with a single command (see repository for multi-GPU deployments)"
    code: |
      curl -sSL https://raw.githubusercontent.com/tiberaicommunity/xpu_tgi/main/quick-deploy.sh \
        | bash -s -- Flan-T5-XXL

  - title: "3. Try Demo UI"
    description: "Launch the demo UI and API documentation interface"
    code: |
      bash ~/xpu_tgi/deploy_ui.sh

  - title: "4. Access Options"
    description: "Multiple ways to access your deployed model (GPU number is automatically assigned, typically starting with gpu0)"
    collapsed: true
    code: |
      # Local Access (same machine):
      curl -X POST http://localhost:8000/flan-t5-xxl/gpu0/generate \
        -H "Authorization: Bearer YOUR_TOKEN" \
        -H "Content-Type: application/json" \
        -d '{"inputs": "Translate to French: Hello, how are you today?"}'

      # Remote Access (evaluation only):
      ./tunnel.sh  # Creates temporary public URL
      # Then use: https://YOUR-TUNNEL-URL/flan-t5-xxl/gpu0/generate

      # SSH Tunnel (recommended):
      ssh -L 8000:localhost:8000 -J guest@guest-ip user@your-vm-ip
      # Then use: http://localhost:8000/flan-t5-xxl/gpu0/generate

  - title: "5. Managing Services"
    description: "Control and monitor your deployment"
    collapsed: true
    code: |
      # Check status
      ./tgi-status.sh

      # Stop services
      ./service_cleanup.sh                    # Stop all services
      ./service_cleanup.sh --gpu N (or all)   # Stop specific GPU or all GPUs

tags:
  - label: "Transformer"
    color: "#3B82F6"
  - label: "512 context"
    color: "#10B981"
  - label: "Instruction"
    color: "#8B5CF6"
  - label: "TGI"
    color: "#EF4444"
  - label: "Google"
    color: "#34D399"
  - label: "T5"
    color: "#6366F1"
  - label: "Translation"
    color: "#F472B6"
